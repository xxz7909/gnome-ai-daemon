# 低显存本地多模态方案（替代云厂商方案）

适用目标：你现在只有一台 `NVIDIA GeForce MX450`（约 2GB 显存）的笔记本，希望优先本地方案。

---

## 1. 核心限制（MX450）

| 参数 | MX450 现实 |
|------|------------|
| 显存 | 2GB（实际可用常见约 1.5GB） |
| 算力 | 约 2.6 TFLOPS FP32 |
| 关键瓶颈 | 显存容量 + 显存带宽 |

结论：

- 不适合运行 Qwen2.5-VL 这类多模态大模型（即使 3B 也常 OOM）
- 必须走 **CPU 推理** 或 **极小模型 + 强量化 + 低频抽帧**

---

## 2. 优先替代方案（先用 1 / 2 / 3）

### 方案 1：Moondream2（首推）

- 规模：约 1.6B
- 优势：轻量，图像理解稳定，容易在 CPU 落地
- 适合：低频次桌面理解、食品描述、巡检告警

建议：

- 输入缩放到 `224x224` 或 `336x336`
- 每秒不超过 1 次 VLM 推理（或场景变化触发）

### 方案 2：Bunny（Mini 版本）

- 规模：约 2B（量化后更小）
- 优势：多模态问答能力较平衡
- 适合：轻度视觉问答 / 分类场景

注意：

- 在 MX450 上仍建议 CPU 或低频推理，不追求逐帧实时

### 方案 3：LLaVA-Phi / MobileVLM v2

| 模型 | 量化后体量（参考） | 适用场景 |
|------|--------------------|----------|
| LLaVA-Phi | ~1.9GB | 基础物体识别 |
| MobileVLM v2 1.7B | ~1.5GB | 移动端优化，速度更好 |

注意：

- 这些体量仍接近 MX450 极限，建议优先 CPU + 降采样策略

---

## 3. 实时监控推荐架构（食品流场景）

```text
摄像头/屏幕 → 抽帧(1fps) → 轻量检测(变化触发) → 关键帧送VLM描述
								   ↓
							  异常/新物品
								   ↓
							 记录日志 / 报警
```

核心策略：

1. **不逐帧跑 VLM**：先做变化检测，只在变化时推理
2. **低分辨率输入**：`224x224` / `336x336`
3. **节流**：固定最小间隔（例如 1s~3s）
4. **仅输出变化**：描述没变化就不重复写日志

---

## 4. 对接当前项目的落地思路

你当前项目主链路是：截图 → 模型决策 → 调用 GNOME API 动作执行。

如果先用本地轻量模型，可按下面方式改造：

- 保留 `agent/loop.py` 的动作执行逻辑
- 把 `agent/model_client.py` 改为本地推理客户端（替换云端 API 调用）
- 在 `loop` 中增加更强节流（例如仅变化触发 + 1~3 秒冷却）

建议参数（先跑稳）：

```bash
export SCREENSHOT_MAX_WIDTH=640
export SCREENSHOT_QUALITY=60
export REALTIME_FPS_INTERVAL=0.5
export ACTION_COOLDOWN_SEC=1.5
export IDLE_SKIP_THRESHOLD=0.04
```

---

## 5. 示例伪代码（变化触发）

```python
import cv2
import time

last_text = ""
last_frame = None

while True:
	frame = read_frame()
	if not scene_changed(frame, last_frame):
		continue

	small = cv2.resize(frame, (224, 224))
	text = local_vlm_describe(small, "描述画面中的食物并估计热量")

	if text != last_text:
		print(text)
		last_text = text

	last_frame = frame
	time.sleep(1.0)
```

---

## 6. 预期性能（现实预估）

| 配置 | 延迟 | 适用场景 |
|------|------|----------|
| MX450 + 轻量模型 + CPU | 3~5 秒/帧 | 低频抽检 |
| CPU + ONNX/OpenVINO 优化 | 0.5~2 秒/帧 | 约 1fps 监控 |

结论：

- 你的机器可做“可用但低频”的本地多模态
- 如果要更高实时性，仍建议后续迁移到更强设备或云端推理

---

## 7. 什么时候回退到云端 API

以下情况建议直接回到云端模型：

- 任务复杂（多步骤 GUI 推理）
- 需要更高准确率
- 需要更接近实时（<1s）

你已经有云端兼容路径，切回只需要恢复环境变量：

```bash
MODEL_API_BASE="https://<provider>/v1" \
MODEL_API_KEY="<api-key>" \
MODEL_NAME="<vision-model>" \
.venv/bin/python run_agent.py --realtime "打开终端并输入 hello world"
```
